{
  "doc-28d6ca9ef56931c4d8cda63d9bbcfd61": {
    "content": "Logistic Regression – Engineering AI Agents\nEngineering AI Agents\nBOOK\nFoundations\nTraining Deep Networks\nPerception\nKinematics\nState Estimation\nLarge Language Models\nMultimodal Reasoning\nTask Planning\nGlobal Planning\nLocal Planning\nMarkov Decision Processes\nReinforcement Learning\nVLA Agents\nCOURSES\nIntroduction to AI\nAI for Robotics\nDeep Learning for Computer Vision\nDATA MINING - BEING PORTED\nMEDIA\nAI for Robotics\nABOUT ME\n \n \nFoundations\nLogistic Regression\nFoundations\nThe Learning Problem\nLinear Regression\nEmpirical Risk Minimization\nOptimization Algorithms\nSGD Example for Linear Regression\nEntropy\nMaximum Likelihood Estimation of a marginal model\nMaximum Likelihood Estimation of Gaussian Parameters\nMaximum Likelihood (ML) Estimation of conditional models\nIntroduction to Classification\nLogistic Regression\nOn this page\nOdds and the Logistic Sigmoid\nBinary case\nOptimizing LR parameters with SGD\nEdit this page\nView source\nReport an issue\nFoundations\nLogistic Regression\nLogistic Regression\nLogistic regression is used in machine learning extensively - every time we need to provide probabilistic semantics to an outcome e.g. predicting the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.), whether an voter will vote for a given party, predicting the probability of failure of a given process, system or product, predicting a customer’s propensity to purchase a product or halt a subscription, predicting the likelihood of a homeowner defaulting on a mortgage.\nOdds and the Logistic Sigmoid\nIf \n\\(\\sigma\\)\n is a probability of an event, then the ratio \n\\(\\frac{\\sigma}{1-\\sigma}\\)\n is the corresponding \nodds\n, the ratio of the event occurring divided by not occurring. For example, if a race horse runs 100 races and wins 25 times and loses the other 75 times, the probability of winning is 25/100 = 0.25 or 25%, but the odds of the horse winning are 25/75 = 0.333 or 1 win to 3 loses. In the binary classification case, the log odds is given by\n\\[ \\mathtt{logit}(\\sigma) = \\alpha = \\ln \\frac{\\sigma}{1-\\sigma} = \\ln \\frac{p(y=1|\\mathbf{x})}{p(y=0 |\\mathbf{x})}\\]\nThe logistic function of any number \n\\(\\alpha\\)\n is given by:\n\\[\\mathtt{logistic}(\\alpha) = \\sigma(\\alpha) = \\mathtt{logit}^{-1}(\\alpha) =  \\frac{1}{1 + \\exp(-\\alpha)} = \\frac{\\exp(\\alpha)}{ \\exp(\\alpha) + 1}\\]\nand is plotted below. It maps its argument to the “probability” space [0,1].\n \nLogistic sigmoid (red)\nThe sigmoid function satisfies the following symmetry:\n\\[\\sigma(-\\alpha) = 1 - \\sigma(\\alpha)\\]\nBinary case\nIf we consider the two class problem, we can write the posterior probability as,\n\\[p(y=1|\\mathbf{x}) = \\frac{p(\\mathbf{x}|y=1) p(y=1)}{p(\\mathbf{x}|y=1) p(y=1) + p(\\mathbf{x}|y=0) p(y=0)} = \\frac{1}{1 + \\exp(-\\alpha)} = \\sigma(\\alpha)\\]\nwhere \n\\(\\alpha = \\ln \\frac{p(\\mathbf{x}|y=1) p(y=1)}{p(\\mathbf{x}|y=0) p(y=0)}\\)\n.\nGiven the posterior distribution above, we have for the specific linear activation,\n\\[p_{model}(y=1|\\mathbf{x}) = \\hat y = \\sigma(\\mathbf{w}^T \\mathbf{x})\\]\nThis model is called logistic regression - despite its name its models a classification task.\nThe figure below shows the corresponding posterior distribution \n\\(p(y=1|\\mathbf{x})\\)\n \nThe class-conditional densities for two classes, denoted red and blue. Here the class-conditional densities \n\\(p(\\mathbf{x}|y=1)\\)\n and \n\\(p(\\mathbf{x}|y=0)\\)\n are Gaussian\n \nThe corresponding posterior probability for the red class, which is given by a logistic sigmoid of a linear function of \n\\(\\mathbf{x}\\)\n.\nBy repeating the classical steps in ML methodology i.e. writing down the expression of the likelihood function (this will now be a product of binomials), we can write down the negative log likelihood function for the binary case as,\n```mcvivk Binary CE Loss \n\\[L(\\mathbf{w}) = L(y, \\hat{y}) = - \\ln p(\\mathbf{y},\\mathbf{w}) = - \\big[ \\sum_{i=1}^m \\{y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) \\} \\big]\\]\nwhich is called \ncross entropy loss function\n - probably the most widely used error function (in classification as well as regression) due to its information theoretic roots. ```\nIts shape is shown in the figure below for an example case where the \n\\(y=1\\)\n. It is also known as log-loss.\ncross-entropy\nCE Loss vs predicted probability for class “1”\nThe behavior of the loss function is telling: it heavily penalizes confident wrong decisions. Notice its exponential rise when the probability \n\\(\\hat{y}\\)\n of the class “1” is close to 0.0 which means that the classifier predicted with high confidence the opposite class that the ground truth. On the other hand, when the probability of \n\\(\\hat y\\)\n is close to 1.0, which means that the classifier predicts the class “1” with confidence, we exhibit minimal loss and in the limit zero loss.\nOptimizing LR parameters with SGD\nMinimizing the Binary CE (log-loss) starts from the negative log likelihood:\n\\[L(\\mathbf{w}) = - \\ln p_{model}(\\mathbf{y},\\mathbf{w}) = - \\big[ \\sum_{i=1}^m \\{y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) \\} \\big]\\]\n\\[ = - \\big[ \\sum_{i=1}^m \\{y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i) \\} \\big]\\]\n\\[ = - \\big[ \\sum_{i=1}^m \\{y_i \\ln \\frac{1}{1+\\exp(-\\alpha)} +(1- y_i) \\ln (1 - \\frac{1}{1+\\exp(-\\alpha)}) \\} \\big] \\]\n\\[ = - \\big[ \\sum_{i=1}^m \\{y_i \\big[ \\ln \\frac{1}{1+\\exp(-\\alpha)} - \\ln (1 - \\frac{1}{1+\\exp(-\\alpha)}) \\big] + \\ln (1 - \\frac{1}{1+\\exp(-\\alpha)} \\} \\big] \\]\n\\[ = - \\big[ \\sum_{i=1}^m \\{y_i \\alpha + \\ln (1+\\exp(\\alpha) \\} \\big] \\]\n\\[ = - \\big[ \\sum_{i=1}^m \\{y_i \\mathbf w^T \\mathbf x  + \\ln (1+\\exp(\\mathbf w^T \\mathbf x) \\} \\big] \\]\nWe need to minimize it with respect to \n\\(\\mathbf{w}\\)\n and this requires calculating its gradient. The gradient can be derived to be:\n\\[\\nabla_\\mathbf w L = \\sum_{i=1}^m (\\hat{y}_i - y_i) x_i\\]\nThe expression above defines the batch gradient decent algorithm. We can then readily convert this algorithm to mini-batch SGD by considering mini-batch updates of size \n\\(m_b\\)\n i.e. instead of \n\\(m\\)\n we sum over \n\\(m_b\\)\n where the later is a hyperparameter.\n \n \nIntroduction to Classification\n \nEdit this page\nView source\nReport an issue"
  },
  "doc-bd3eac181a641dbc1d9c867848139be0": {
    "content": "Word2Vec from scratch – Engineering AI Agents\nEngineering AI Agents\nBOOK\nFoundations\nTraining Deep Networks\nPerception\nKinematics\nState Estimation\nLarge Language Models\nMultimodal Reasoning\nTask Planning\nGlobal Planning\nLocal Planning\nMarkov Decision Processes\nReinforcement Learning\nVLA Agents\nCOURSES\nIntroduction to AI\nAI for Robotics\nDeep Learning for Computer Vision\nDATA MINING - BEING PORTED\nMEDIA\nAI for Robotics\nABOUT ME\n \n \nLarge Language Models\nWord2Vec from scratch\nLarge Language Models\nIntroduction to NLP Pipelines\nText Tokenization\nWord2Vec Embeddings\nWord2Vec from scratch\nWord2Vec Tensorflow Tutorial\nIntroduction to Recurrent Neural Networks (RNN)\nSimple RNN\nTime Series Prediction using RNNs\nThe Long Short-Term Memory (LSTM) Architecture\nLanguage Models\nRNN Language Models\nExample of an RNN Language Model\nRNN-based Neural Machine Translation\nCharacter-level recurrent sequence-to-sequence model\nThe BLEU Score\nAttention in RNN-based NMT\nTransformers and Self-Attention\nSingle-head self-attention\nMulti-head self-attention\nMultilayer Perceptron (MLP)\nPositional Embeddings\nLLM Inference\nLarge Language Models\nWord2Vec from scratch\nWord2Vec from scratch\nThis \nself-contained implementation\n is instructive and you should \ngo through it\n to understand the word2vec embedding.\nimport\n numpy \nas\n np\nimport\n re\nfrom\n collections \nimport\n defaultdict\nclass\n word2vec():\n    \ndef\n \n__init__\n (\nself\n):\n        \nself\n.n \n=\n settings[\n'n'\n]\n        \nself\n.eta \n=\n settings[\n'learning_rate'\n]\n        \nself\n.epochs \n=\n settings[\n'epochs'\n]\n        \nself\n.window \n=\n settings[\n'window_size'\n]\n        \npass\n    \n    \n    \n# GENERATE TRAINING DATA\n    \ndef\n generate_training_data(\nself\n, settings, corpus):\n        \n# GENERATE WORD COUNTS\n        word_counts \n=\n defaultdict(\nint\n)\n        \nfor\n row \nin\n corpus:\n            \nfor\n word \nin\n row:\n                word_counts[word] \n+=\n \n1\n        \nself\n.v_count \n=\n \nlen\n(word_counts.keys())\n        \n# GENERATE LOOKUP DICTIONARIES\n        \nself\n.words_list \n=\n \nsorted\n(\nlist\n(word_counts.keys()),reverse\n=\nFalse\n)\n        \nself\n.word_index \n=\n \ndict\n((word, i) \nfor\n i, word \nin\n \nenumerate\n(\nself\n.words_list))\n        \nself\n.index_word \n=\n \ndict\n((i, word) \nfor\n i, word \nin\n \nenumerate\n(\nself\n.words_list))\n        training_data \n=\n []\n        \n# CYCLE THROUGH EACH SENTENCE IN CORPUS\n        \nfor\n sentence \nin\n corpus:\n            sent_len \n=\n \nlen\n(sentence)\n            \n# CYCLE THROUGH EACH WORD IN SENTENCE\n            \nfor\n i, word \nin\n \nenumerate\n(sentence):\n                \n                \n#w_target  = sentence[i]\n                w_target \n=\n \nself\n.word2onehot(sentence[i])\n                \n# CYCLE THROUGH CONTEXT WINDOW\n                w_context \n=\n []\n                \nfor\n j \nin\n \nrange\n(i\n-\nself\n.window, i\n+\nself\n.window\n+\n1\n):\n                    \nif\n j\n!=\ni \nand\n j\n<=\nsent_len\n-\n1\n \nand\n j\n>=\n0\n:\n                        w_context.append(\nself\n.word2onehot(sentence[j]))\n                training_data.append([w_target, w_context])\n        \nreturn\n np.array(training_data)\n    \n# SOFTMAX ACTIVATION FUNCTION\n    \ndef\n softmax(\nself\n, x):\n        e_x \n=\n np.exp(x \n-\n np.\nmax\n(x))\n        \nreturn\n e_x \n/\n e_x.\nsum\n(axis\n=\n0\n)\n    \n# CONVERT WORD TO ONE HOT ENCODING\n    \ndef\n word2onehot(\nself\n, word):\n        word_vec \n=\n [\n0\n \nfor\n i \nin\n \nrange\n(\n0\n, \nself\n.v_count)]\n        word_index \n=\n \nself\n.word_index[word]\n        word_vec[word_index] \n=\n \n1\n        \nreturn\n word_vec\n    \n# FORWARD PASS\n    \ndef\n forward_pass(\nself\n, x):\n        h \n=\n np.dot(\nself\n.w1.T, x)\n        u \n=\n np.dot(\nself\n.w2.T, h)\n        y_c \n=\n \nself\n.softmax(u)\n        \nreturn\n y_c, h, u\n                \n    \n# BACKPROPAGATION\n    \ndef\n backprop(\nself\n, e, h, x):\n        dl_dw2 \n=\n np.outer(h, e)  \n        dl_dw1 \n=\n np.outer(x, np.dot(\nself\n.w2, e.T))\n        \n# UPDATE WEIGHTS\n        \nself\n.w1 \n=\n \nself\n.w1 \n-\n (\nself\n.eta \n*\n dl_dw1)\n        \nself\n.w2 \n=\n \nself\n.w2 \n-\n (\nself\n.eta \n*\n dl_dw2)\n        \npass\n    \n# TRAIN W2V model\n    \ndef\n train(\nself\n, training_data):\n        \n# INITIALIZE WEIGHT MATRICES\n        \nself\n.w1 \n=\n np.random.uniform(\n-\n0.8\n, \n0.8\n, (\nself\n.v_count, \nself\n.n))     \n# embedding matrix\n        \nself\n.w2 \n=\n np.random.uniform(\n-\n0.8\n, \n0.8\n, (\nself\n.n, \nself\n.v_count))     \n# context matrix\n        \n        \n# CYCLE THROUGH EACH EPOCH\n        \nfor\n i \nin\n \nrange\n(\n0\n, \nself\n.epochs):\n            \nself\n.loss \n=\n \n0\n            \n# CYCLE THROUGH EACH TRAINING SAMPLE\n            \nfor\n w_t, w_c \nin\n training_data:\n                \n# FORWARD PASS\n                y_pred, h, u \n=\n \nself\n.forward_pass(w_t)\n                \n                \n# CALCULATE ERROR\n                EI \n=\n np.\nsum\n([np.subtract(y_pred, word) \nfor\n word \nin\n w_c], axis\n=\n0\n)\n                \n# BACKPROPAGATION\n                \nself\n.backprop(EI, h, w_t)\n                \n# CALCULATE LOSS\n                \nself\n.loss \n+=\n \n-\nnp.\nsum\n([u[word.index(\n1\n)] \nfor\n word \nin\n w_c]) \n+\n \nlen\n(w_c) \n*\n np.log(np.\nsum\n(np.exp(u)))\n                \n#self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))\n                \n            \nprint\n(\n'EPOCH:'\n,i, \n'LOSS:'\n, \nself\n.loss)\n        \npass\n    \n# input a word, returns a vector (if available)\n    \ndef\n word_vec(\nself\n, word):\n        w_index \n=\n \nself\n.word_index[word]\n        v_w \n=\n \nself\n.w1[w_index]\n        \nreturn\n v_w\n    \n# input a vector, returns nearest word(s)\n    \ndef\n vec_sim(\nself\n, vec, top_n):\n        \n# CYCLE THROUGH VOCAB\n        word_sim \n=\n {}\n        \nfor\n i \nin\n \nrange\n(\nself\n.v_count):\n            v_w2 \n=\n \nself\n.w1[i]\n            theta_num \n=\n np.dot(vec, v_w2)\n            theta_den \n=\n np.linalg.norm(vec) \n*\n np.linalg.norm(v_w2)\n            theta \n=\n theta_num \n/\n theta_den\n            word \n=\n \nself\n.index_word[i]\n            word_sim[word] \n=\n theta\n        words_sorted \n=\n \nsorted\n(word_sim.items(), key\n=\nlambda\n item: item[\n1\n], reverse\n=\nTrue\n)\n        \nfor\n word, sim \nin\n words_sorted[:top_n]:\n            \nprint\n(word, sim)\n            \n        \npass\n    \n# input word, returns top [n] most similar words\n    \ndef\n word_sim(\nself\n, word, top_n):\n        \n        w1_index \n=\n \nself\n.word_index[word]\n        v_w1 \n=\n \nself\n.w1[w1_index]\n        \n# CYCLE THROUGH VOCAB\n        word_sim \n=\n {}\n        \nfor\n i \nin\n \nrange\n(\nself\n.v_count):\n            v_w2 \n=\n \nself\n.w1[i]\n            theta_num \n=\n np.dot(v_w1, v_w2)\n            theta_den \n=\n np.linalg.norm(v_w1) \n*\n np.linalg.norm(v_w2)\n            theta \n=\n theta_num \n/\n theta_den\n            word \n=\n \nself\n.index_word[i]\n            word_sim[word] \n=\n theta\n        words_sorted \n=\n \nsorted\n(word_sim.items(), key\n=\nlambda\n item: item[\n1\n], reverse\n=\nTrue\n)\n        \nfor\n word, sim \nin\n words_sorted[:top_n]:\n            \nprint\n(word, sim)\n            \n        \npass\nsettings \n=\n {}\nsettings[\n'n'\n] \n=\n \n5\n                   \n# dimension of word embeddings\nsettings[\n'window_size'\n] \n=\n \n2\n         \n# context window +/- center word\nsettings[\n'min_count'\n] \n=\n \n0\n           \n# minimum word count\nsettings[\n'epochs'\n] \n=\n \n30000\n           \n# number of training epochs\nsettings[\n'neg_samp'\n] \n=\n \n10\n           \n# number of negative words to use during training\nsettings[\n'learning_rate'\n] \n=\n \n0.01\n    \n# learning rate\nnp.random.seed(\n0\n)                   \n# set the seed for reproducibility\ncorpus \n=\n [[\n'the'\n,\n'quick'\n,\n'brown'\n,\n'fox'\n,\n'jumped'\n,\n'over'\n,\n'the'\n,\n'lazy'\n,\n'dog'\n]]\n# INITIALIZE W2V MODEL\nw2v \n=\n word2vec()\n# generate training data\ntraining_data \n=\n w2v.generate_training_data(settings, corpus)\n# train word2vec model\nw2v.train(training_data)\n/tmp/ipykernel_482824/2817892463.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return np.array(training_data)\nEPOCH: 0 LOSS: 68.37096376709992\nEPOCH: 1 LOSS: 67.88313773129582\nEPOCH: 2 LOSS: 67.43127505997181\nEPOCH: 3 LOSS: 67.0112049943476\nEPOCH: 4 LOSS: 66.61935410862232\nEPOCH: 5 LOSS: 66.25264389886307\nEPOCH: 6 LOSS: 65.9084083928434\nEPOCH: 7 LOSS: 65.5843274465069\nEPOCH: 8 LOSS: 65.2783724111086\nEPOCH: 9 LOSS: 64.98876161794144\nEPOCH: 10 LOSS: 64.71392370243775\nEPOCH: 11 LOSS: 64.45246722531292\nEPOCH: 12 LOSS: 64.20315538093011\nEPOCH: 13 LOSS: 63.96488483826672\nEPOCH: 14 LOSS: 63.736667956900455\nEPOCH: 15 LOSS: 63.51761777345709\nEPOCH: 16 LOSS: 63.30693527348039\nEPOCH: 17 LOSS: 63.103898557554714\nEPOCH: 18 LOSS: 62.90785358463229\nEPOCH: 19 LOSS: 62.71820623435013\nEPOCH: 20 LOSS: 62.5344154770562\nEPOCH: 21 LOSS: 62.355987477886515\nEPOCH: 22 LOSS: 62.18247049153716\nEPOCH: 23 LOSS: 62.01345042888875\nEPOCH: 24 LOSS: 61.8485469965609\nEPOCH: 25 LOSS: 61.687410326726464\nEPOCH: 26 LOSS: 61.529718027831066\nEPOCH: 27 LOSS: 61.375172597813304\nEPOCH: 28 LOSS: 61.22349915045999\nEPOCH: 29 LOSS: 61.07444341301829\nEPOCH: 30 LOSS: 60.92776995941049\nEPOCH:"
  },
  "doc-e639a06cd74f8c691d1564818ca65413": {
    "content": "Fast RCNN Object Detection – Engineering AI Agents\nEngineering AI Agents\nBOOK\nFoundations\nTraining Deep Networks\nPerception\nKinematics\nState Estimation\nLarge Language Models\nMultimodal Reasoning\nTask Planning\nGlobal Planning\nLocal Planning\nMarkov Decision Processes\nReinforcement Learning\nVLA Agents\nCOURSES\nIntroduction to AI\nAI for Robotics\nDeep Learning for Computer Vision\nDATA MINING - BEING PORTED\nMEDIA\nAI for Robotics\nABOUT ME\n \n \nFast RCNN Object Detection\nIntroduction to Convolutional Neural Networks\nCNN Layers\nCNN Example Architectures\nUsing ConvNets with Small Datasets\nVisualizing what convnets learn\nFeature Extraction via Residual Networks\nIntroduction to Scene Understanding\nObject Detection\nObject Detection and Semantic Segmentation Metrics\nRegion-CNN (RCNN) Object Detection\nFast RCNN Object Detection\nFaster RCNN Object Detection\nYou Only Look Once (YOLO)\nMask R-CNN Semantic Segmentation\nMask R-CNN Demo\nMask R-CNN - Inspect Training Data\nMask R-CNN - Inspect Trained Model\nMask R-CNN - Inspect Weights of a Trained Model\nMaskRCNN Inference\nDetectron2 Beginner’s Tutorial\nOn this page\nSelective search\nModified backbone network\nRoI Pooling Layer\nFully connected classification and regression heads\nFinetuning\nInference\nEdit this page\nView source\nReport an issue\nFast RCNN Object Detection\nFast-RCNN\n is the 2nd generation RCNN that aimed to improve inference time. Apart from the complex training process of RCNN that involved multiple phases across neural and non-neural predictors, its inference involved a forward pass for each of the 2000 proposals through the backbone CNN.\nThe Fast RCNN architecture is shown below:\nFigure 1: Fast RCNN Architecture\nSelective search\nThis is identical to the method we met in RCNN and produces produces region proposals - typically up to 2000 per image.\nModified backbone network\nA fully convolutional network network processes the \nwhole image\n and has several convolutional and max-pooling layers (experiments were done for 5-13 conv and 5 max pooling layers) to produce a feature map.\nRoI Pooling Layer\nWhat is an RoI? An RoI is a rectangular window and is mapped (projected) from the proposals produced by selective search (pixel-space) into the feature map. This projection defines for each proposal a four-tuple (x, y, h, w) that specifies its top-left corner (x, y), its height and width (h, w).\nThe pooling layer uses max pooling to convert the features inside any valid RoI into a small feature map with a fixed spatial extent of H × W (e.g.7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI.\nRoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied \nindependently\n to each feature map channel, as in standard max pooling.\nTo accommodate the RoI pooling layer we swap out the last pooling layer of the CNN and endure that the first fully connected layer is dimensionally compatible to accept the RoI pooling layer output.\nBottom line is that \nfor each proposal\n, a Region of Interest (RoI) pooling layer extracts a fixed-length \nfeature vector from the feature map\n.\nFully connected classification and regression heads\nEach feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output heads:\nThe classification head that produces softmax probability estimates over K object classes plus a catch-all “background” class,\nThe Regression head that outputs four (4) real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.\nFinetuning\nThe Fast RCNN network is finetuned end-to-end using a multi-task loss function that combines the losses of the classification and bounding-box regression tasks. The loss function is given by:\n\\[L(\\hat y, y, t^y, v) = L_{cls}(\\hat y, y) + \\lambda[y \\geq 1]L_{loc}(t^y, v)\\]\nwhere: - \n\\(\\hat y\\)\n is the predicted probability of the ground-truth class,\n\\(y\\)\n is the ground-truth class,\n\\(t^y\\)\n is the predicted offsets of the predicted bounding-box for class \n\\(y\\)\n,\n\\(v\\)\n is the ground truth bounding-box for class \n\\(y\\)\n,\n\\(L_{cls}\\)\n is the log loss for classification,\n\\(L_{loc}\\)\n is the smooth L1 loss for bounding-box regression,\n\\([y \\geq 1]\\)\n is an indicator function that is 1 if \n\\(y \\geq 1\\)\n and 0 otherwise, and\n\\(\\lambda\\)\n is a hyper-parameter that balances the two loss terms.\n\\(L_{cls}\\)\n is defined as the usual CE:\n\\[L_{cls}(\\hat y, y) = -\\log(\\hat y_y)\\]\nwhere \n\\(\\hat y_y\\)\n is the predicted probability of the ground-truth class.\nwhile \n\\(L_{loc}\\)\n is defined as:\n\\[L_{loc}(t^y, v) = \\sum_{i \\in {x, y, w, h}} \\mathtt{smooth}_{L1}(t^y_i - v_i)\\]\nwhere\n\\[\n\\mathtt{smooth}_{L1}(x) =\n\\begin{cases}\n0.5 x^2, & \\text{if } |x| < 1 \\\\\n|x| - 0.5, & \\text{otherwise}\n\\end{cases}\n\\]\nInference\nThe set of proposals is produced by the same selective search algorithm used in RCNN and its similarly up to 2000 per image.\nA Fast RCNN network takes as input an entire image and the set of proposals \n\\(R\\)\n. For each proposal the network produces a set of K+1 posterior probabilities and 4K predicted bounding box offsets. We then perform Non-Max Suppression is maintained just like in RCNN to produce the final detection.\n \n \nRegion-CNN (RCNN) Object Detection\nFaster RCNN Object Detection\n \n \nEdit this page\nView source\nReport an issue"
  },
  "doc-6d67fadcc0a8c6bd91a1e45446c1b4e0": {
    "content": "Linear Regression – Engineering AI Agents\nEngineering AI Agents\nBOOK\nFoundations\nTraining Deep Networks\nPerception\nKinematics\nState Estimation\nLarge Language Models\nMultimodal Reasoning\nTask Planning\nGlobal Planning\nLocal Planning\nMarkov Decision Processes\nReinforcement Learning\nVLA Agents\nCOURSES\nIntroduction to AI\nAI for Robotics\nDeep Learning for Computer Vision\nDATA MINING - BEING PORTED\nMEDIA\nAI for Robotics\nABOUT ME\n \n \nFoundations\nLinear Regression\nFoundations\nThe Learning Problem\nLinear Regression\nEmpirical Risk Minimization\nOptimization Algorithms\nSGD Example for Linear Regression\nEntropy\nMaximum Likelihood Estimation of a marginal model\nMaximum Likelihood Estimation of Gaussian Parameters\nMaximum Likelihood (ML) Estimation of conditional models\nIntroduction to Classification\nLogistic Regression\nOn this page\nBias and Variance Decomposition during the training process\nEdit this page\nView source\nReport an issue\nFoundations\nLinear Regression\nLinear Regression\nNow that we have introduced somewhat more formally the learning problem and its notation lets us study a simple but instructive regression problem that is known in the statistics literature as \nshrinkage\n.\nSuppose that we are given the training set \n\\(\\mathbf{x} = \\{x_1,...,x_m\\}\\)\n together with their labels, the vectors \n\\(\\mathbf{y}\\)\n. We need to construct a model such that a \nsuitably chosen\n loss function is minimized for a \ndifferent\n set of input data, the so-called test set. The ability to correctly \npredict\n when observing the test set, is called \ngeneralization\n.\n \nTraining Dataset (m=10) for the Regression Model. The green curve is the uknown target function. Please replace the label axis denoted by \n\\(t\\)\n with \n\\(y\\)\n to make it compatible with our notation.\nSince the output \n\\(y\\)\n is a continuous variable then the supervised learning problem is called a regression problem (otherwise its a classification problem). The synthetic dataset is generated by the function \n\\(\\sin(2 \\pi x) + ϵ\\)\n where \n\\(x\\)\n is a uniformly distributed random variable and \n\\(ϵ\\)\n is \n\\(N(\\mu=0.0, \\sigma^2=0.3)\\)\n. This target function is \ncompletely unknown\n to us - we just mention it here just to provide a visual clue of what the hypothesis set needs to aspire to.\n!\npip install git\n+\nhttps:\n//\ngithub.com\n/\npantelis\n-\nclasses\n/\nPRML.git\n#egg=prml\nimport\n seaborn \nas\n sns\n# Apply the default theme\nsns.set_theme()\nCollecting prml\n  Cloning https://github.com/pantelis-classes/PRML.git to /tmp/pip-install-u56rgxv9/prml_549867dfcd3f444992ead201b926f214\n  Running command git clone --filter=blob:none --quiet https://github.com/pantelis-classes/PRML.git /tmp/pip-install-u56rgxv9/prml_549867dfcd3f444992ead201b926f214\n  Resolved https://github.com/pantelis-classes/PRML.git to commit 6c7ef85da419a644a4a4feb7ab538d2f4f15d46b\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /workspaces/engineering-ai-agents/.venv/lib/python3.11/site-packages (from prml) (1.26.4)\nRequirement already satisfied: scipy in /workspaces/engineering-ai-agents/.venv/lib/python3.11/site-packages (from prml) (1.16.0)\nBuilding wheels for collected packages: prml\n  Building wheel for prml (setup.py) ... done\n  Created wheel for prml: filename=prml-0.0.1-py3-none-any.whl size=88378 sha256=501862446168ef175115dd98c3903afa9b51c2aaedd7ac4488e4e1d3312c770a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-55ore5eo/wheels/be/c7/69/639b72a88940bc45c2c531c36b623139c05036dab44d33b761\nSuccessfully built prml\nInstalling collected packages: prml\nSuccessfully installed prml-0.0.1\n[\nnotice\n]\n A new release of pip is available: \n24.0\n -> \n25.1.1\n[\nnotice\n]\n To update, run: \npip install --upgrade pip\nimport\n numpy \nas\n np\nfrom\n scipy.stats \nimport\n multivariate_normal\nimport\n matplotlib.pyplot \nas\n plt\n%\nmatplotlib inline\nfrom\n prml.preprocess \nimport\n GaussianFeature, PolynomialFeature, SigmoidalFeature\nfrom\n prml.linear \nimport\n BayesianRegression, EmpiricalBayesRegression, LinearRegression, RidgeRegression\nnp.random.seed(\n1234\n)\ndef\n create_toy_data(func, sample_size, std, domain\n=\n[\n0\n, \n1\n]):\n    x \n=\n np.linspace(domain[\n0\n], domain[\n1\n], sample_size)\n    np.random.shuffle(x)\n    y \n=\n func(x) \n+\n np.random.normal(scale\n=\nstd, size\n=\nx.shape)\n    \nreturn\n x, y\ndef\n sinusoidal(x):\n    \nreturn\n np.sin(\n2\n \n*\n np.pi \n*\n x)\nx_train, y_train \n=\n create_toy_data(sinusoidal, \n10\n, \n0.25\n)\nx_test \n=\n np.linspace(\n0\n, \n1\n, \n100\n)\ny_test \n=\n sinusoidal(x_test)\nplt.figure(figsize\n=\n[\n10\n, \n8\n])\nplt.scatter(x_train, y_train, facecolor\n=\n\"none\"\n, edgecolor\n=\n\"b\"\n, s\n=\n50\n, label\n=\n\"training data\"\n)\nplt.plot(x_test, y_test, label\n=\n\"$\n\\\nsin(2\n\\\npi x)$\"\n)\nplt.plot(x_test, y_test, \n\"-g\"\n, label\n=\n\"Target Function\"\n)\nplt.legend()\nplt.show()\nLet us now pick the hypothesis set that corresponds in general to the following sum,\n\\[g(\\mathbf{w},\\mathbf{x}) = w_0 + \\sum_{j=1}^{M-1}w_j \\phi_j(\\mathbf x) = \\mathbf w^T \\mathbf \\phi(\\mathbf x)\\]\nwhere \n\\(\\phi_j(\\mathbf x)\\)\n are know as \nbasis functions\n. A set of Polynomial, Gaussian and Sigmoidal basis functions are plotted below.\nx \n=\n np.linspace(\n-\n1\n, \n1\n, \n100\n)\nX_polynomial \n=\n PolynomialFeature(\n11\n).transform(x[:, \nNone\n])\nX_gaussian \n=\n GaussianFeature(np.linspace(\n-\n1\n, \n1\n, \n11\n), \n0.1\n).transform(x)\nX_sigmoidal \n=\n SigmoidalFeature(np.linspace(\n-\n1\n, \n1\n, \n11\n), \n10\n).transform(x)\nplt.figure(figsize\n=\n(\n20\n, \n5\n))\nfor\n i, X \nin\n \nenumerate\n([X_polynomial, X_gaussian, X_sigmoidal]):\n    plt.subplot(\n1\n, \n3\n, i \n+\n \n1\n)\n    \nfor\n j \nin\n \nrange\n(\n12\n):\n        plt.plot(x, X[:, j])\ntxt \n=\n \n\"Polynomial, Gaussian and Sigmoidal basis functions\"\nplt.figtext(\n0.5\n, \n0.01\n, txt, wrap\n=\nTrue\n, horizontalalignment\n=\n\"center\"\n, fontsize\n=\n12\n)\nText(0.5, 0.01, 'Polynomial, Gaussian and Sigmoidal basis functions')\nOur job is to find \n\\(\\mathbf{w}\\)\n such that the polynomial expansion above fits the data we are given - as we will see there are multiple hypothesis that can satisfy this requirement. Consistent with the block diagram we need to define a metric, an figure of merit or loss function in fact, that is also a common metric in regression problems of this nature. This is the Mean Squared Error (MSE) function.\n\\[L(\\mathbf{w}) = \\frac{1}{m} \\sum_{i=1}^m \\{(g(\\mathbf{w},x_i)-y_i)\\}^2\\]\n .\nLoss Function\nFigure 1: The loss function chosen for this regression problem, corresponds to the sum of the squares of the displacements of each data point and our hypothesis. The sum of squares in the case of Gaussian errors gives raise to an (unbiased) Maximum Likelihood estimate of the model parameters. Contrast this to sum of absolute differences.\nNow our job has become to choose two things: the weight vector \n\\(\\mathbf{w^*}\\)\n \nand\n \n\\(M\\)\n the order of the polynomial. \nBoth\n define our hypothesis. If you think about it, the order \n\\(M\\)\n defines the model complexity in the sense that the larger \n\\(M\\)\n becomes the more the number of weights we need to estimate and store. Obviously this is a trivial example and storage is not a concern here but treat this example as instructive for that it applies in far for complicated settings.\n \n \n \nVarious final hypotheses.\n \nObviously you can reduce the training error to almost zero by selecting a model that is complicated enough (M=9) to perfectly fit the training data (if m is small).\nLoss Function\nBut this is not what you want to do. Because when met with test data, the model will perform far worse than a less complicated model that is closer to the true model (e.g. M=3). This is a central observation in statistical learning called \noverfitting\n. In addition, you may not have the time to iterate over M (very important in online learning settings).\nM \n=\n \n9\n# Pick one of the three features below\nfeature \n=\n PolynomialFeature(M)\n# feature = GaussianFeature(np.linspace(0, 1, M), 0.1)\n# feature = SigmoidalFeature(np.linspace(0, 1, M), 10)\nX_train \n=\n feature.transform(x_train)\nX_test \n=\n feature.transform(x_test)\nmodel \n=\n LinearRegression()\nmodel.fit(X_train, y_train)\nplt.figure(figsize\n=\n[\n10\n, \n8\n])\nplt.plot(model.w)\nplt.xlabel(\n\"index of $w$\"\n)\nplt.ylabel(\n\"$w$\"\n)\ny, y_std \n=\n model.predict(X_test, return_std\n=\nTrue\n)\nplt.figure(figsize\n=\n[\n10\n, \n8\n])\nplt.scatter(x_train, y_train, facecolor\n=\n\"none\"\n, edgecolor\n=\n\"b\"\n, s\n=\n50\n, label\n=\n\"training data\"\n)\nplt.plot(x_test, y, label\n=\n\"mean\"\n)\nplt.fill_between(x_test, y \n-\n y_std, y \n+\n y_std, color\n=\n\"orange\"\n, alpha\n=\n0.5\n, label\n=\n\"std.\"\n)\nplt.legend()\nplt.xlabel(\n\"$x$\"\n)\nplt.ylabel(\n\"$y$\"\n)\nplt.show()\nTo avoid overfitting we have multiple strategies. One straightforward one is evident by observing the wild oscillations of the \n\\(\\mathbf{w}\\)\n elements as the model complexity increases. We can penalize such oscillations by introducing the \n\\(l_2\\)\n norm of \n\\(\\mathbf{w}\\)\n in our loss function.\n\\[L(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^m \\{(g(\\mathbf{w},x_i)-y_i)\\}^2 + \\frac{\\lambda}{2} ||\\mathbf{w}||^2\\]\nThis type of solution is called \nregularization\n and because we effectively shrink the weight dynamic range it is also called in statistics shrinkage or ridge regression. We have introduced a new parameter \n\\(\\lambda\\)\n that regulates the relative importance of the penalty term as compared to the MSE. This parameter together with the polynomial order is what we call \nhyperparameters\n and we need to optimize them as both are needed for the determination of our final hypothesis \n\\(g\\)\n.\nThe graph below show the results of each search iteration on the \n\\(\\lambda\\)\n hyperparameter.\nLoss Function\nmodel \n=\n RidgeRegression(alpha\n=\n1e-3\n)\nmodel.fit(X_train, y_train)\ny \n=\n model.predict(X_test)\nplt.figure(figsize\n=\n[\n10\n, \n8\n])\nplt.scatter(x_train, y_train, facecolor\n=\n\"none\"\n, edgecolor\n=\n\"b\"\n, s\n=\n50\n, label\n=\n\"training data\"\n)\nplt.plot(x_test, y, label\n=\n\"Regularized Final Hypothesis\"\n)\nplt.legend()\nplt.show()\nBias and Variance Decomposition during the training process\nApart from the composition of the generalization error for various model capacities, it is interesting to make some general comments regarding the decomposition of the generalization error (also known as empirical risk) during training. Early in training the bias is large because the network output is far from the design function. The variance is very small because the data has had little influence yet. Late in training the bias is small because the network has learned the underlying function. However if train for too long then the network will also have learned the noise specific to the dataset (overfitting). In such case the variance will be large because the noise varies between training and test datasets.\nNote\nThis notebook makes extensive use of the \nPattern Recognition for Machine Learning python package\n.\n \n \nThe Learning Problem\nEmpirical Risk Minimization\n \n \nEdit this page\nView source\nReport an issue"
  },
  "doc-a9b2c1fa60b55e13d5422bf6b4720609": {
    "content": "index – Engineering AI Agents\nEngineering AI Agents\nBOOK\nFoundations\nTraining Deep Networks\nPerception\nKinematics\nState Estimation\nLarge Language Models\nMultimodal Reasoning\nTask Planning\nGlobal Planning\nLocal Planning\nMarkov Decision Processes\nReinforcement Learning\nVLA Agents\nCOURSES\nIntroduction to AI\nAI for Robotics\nDeep Learning for Computer Vision\nDATA MINING - BEING PORTED\nMEDIA\nAI for Robotics\nABOUT ME\n \n \nLogical Reasoning\nLogical Agents\nIntroduction\nSyllabus\nRules, rule the world\nAI Agents\nFoundations\nFoundations\nDeep Neural Networks\nTraining Deep Neural Networks\nPerception\n2D Perception\nRecursive State Estimation\nState Estimation\nLarge Language Models\nLarge Language Models\nLogical Reasoning\nAutomated Reasoning\nWorld Models\nLogical Inference\nLogical Agents\nPlanning\nTask Planning\nActing - Markov Decision Processes\nMarkov Decision Processes\nActing - Reinforcement Learning\nReinforcement Learning\nMath Background\nMath for ML Textbook\nProbability Basics\nLinear Algebra for Machine Learning\nCalculus\nResources\nSubmitting Your Assignment / Project\nYour Programming Environment\nLearn Python\nAssignments\nAssignment 1 (undergrad)\nAssignment 1 (grad)\nAssignment 2\nAssignment 3\nAssignment 4\nProject\nIntroducing Erica - Your AI Tutor\nOn this page\nLogical Agents\nEnhancing the KB\nEdit this page\nView source\nReport an issue\nLogical Reasoning\nLogical Agents\nLogical Agents\nIn this chapter we see how agents equipped with the ability to represent internally the state of the environment and reason about the effectiveness of possible actions using propositional logic, can construct plans that are guaranteed to achieve their goals. We start with the KB we developed in wumpus world and enhance it by introducing representations that avoid looking back at the whole percept history to make an inference about the state of the environment currently.\nEnhancing the KB\nWe add a couple of sentences in the KB we had developed,focusing on axioms (facts or rules that evaluate to TRUE). When these axioms involve variables that they never change we call such variables \natemporal\n. This is shown in the next table.\nSentence\nDescription\nKB\n\\(R_1\\)\nThere is no pit in cel [1,1]\n\\(\\neg P_{1,1}\\)\n\\(R_2\\)\nThe cell [1,1] is breezy if and only if there is a pit in the neighboring cell.\n\\(B_{1,1} ⇔ (P_{1,2} \\lor P_{2,1})\\)\n\\(R_3\\)\nThe cell [2,1] is breezy if and only if there is a pit in the neighboring cell.\n\\(B_{2,1} ⇔ (P_{1,1} \\lor P_{2,2} \\lor P_{3,1})\\)\n\\(R_4\\)\nThe cell [1,1] is stenchy if and only if there is a wumpus in the neighboring cell.\n\\(S_{1,1} ⇔ (W_{1,2} \\lor W_{2,1})\\)\n\\(R_5\\)\nThere is only one wumpus in this world.\n\\(R_{5-1}\\)\nThere is at least one wumpus\n\\(W_{1,1} \\lor W_{1,2} \\lor ... \\lor W_{4,4}\\)\n\\(R_{5-2}\\)\nThere is at most one wumpus\n\\(R_{5-2-1}\\)\n\\(\\neg W_{1,1} \\lor \\neg W_{1,2}\\)\n\\(R_{5-2-2}\\)\n\\(\\neg W_{1,1} \\lor \\neg W_{1,3}\\)\n…\n…\n\\(R_{5-2-N}\\)\n\\(\\neg W_{4,3} \\lor \\neg W_{4,4}\\)\nWith respect to percepts and and in fact with everything in the world that is dynamic, we introduce the notion of \ntime\n (\n\\(t\\)\n) and index all such variables accordingly. We do so to avoid the situation where we have conflicts in the KB where while in cell [1,2] we store in the KB \n\\(\\mathtt{Stench}\\)\n and in another instance e.g. in cell [2,2] we store \n\\(\\neg \\mathtt{Stench}\\)\n. These type of variables are called some times called \nfluent\n but effectively these are the usual mutable state variables. Location as we have seen is a primary variable that is carried by the environment state. We can then define an associated fluent \n\\(L_{x,y}^t\\)\n and store the following sentences in the KB.\nSentence\nDescription\nKB\n\\(R_6\\)\nif an agent is in cell [x,y] at \n\\(t\\)\n and perceives [Stench, Breeze, Glitter, Bump, Scream]=[None, Breeze, None, None, None] then the cell is a Breeze cell\n\\(L_{x,y}^t \\implies (\\mathtt{Breeze}^t ⇔ B_{x,y})\\)\n\\(R_7\\)\nif an agent is in cell [x,y] at \n\\(t\\)\n and perceives [Stench, Breeze, Glitter, Bump, Scream]=[Stench, None, None, None, None] then the cell is a Stench cell\n\\(L_{x,y}^t \\implies (\\mathtt{Stench}^t ⇔ S_{x,y})\\)\nSimilar to percepts, actions also are taken at specific times \n\\(t\\)\n. By convention, we first receive a percept at \n\\(t\\)\n, then take an action during the same time step \n\\(t\\)\n and then the environment transitions to another state at \n\\(t+1\\)\n. To specify the transition model that describes how the world changes we use \neffect axioms\n as follows:\nSentence\nDescription\nKB\n\\(R_8\\)\nif an agent is in cell [1,1] at \n\\(t=0\\)\n and is \n\\(\\mathtt{FacingEast}\\)\n and decides to move \n\\(\\mathtt{Forward}\\)\n it will transition to \n\\(L_{2,1}\\)\n in the next time step\n\\(L_{1,1}^0 \\land \\mathtt{FacingEast}^0 \\land \\mathtt{Forward}^0 \\implies (L_{2,1}^1 \\land \\neg L{1,1}^1)\\)\nThere lies a serious issue. There is a combinatorial explosion problem as we have to repeat sentences like the \n\\(R_8\\)\n for each of the possible \n\\(T\\)\n time steps into the future that include actions. For a single \n\\(\\mathtt{Forward}\\)\n action we need to consider \n\\(4 x T x n^2\\)\n possibilities, where 4 are the possible agent orientations and \n\\(n^2\\)\n is the number cells in the wumpus world! But there are more bad news. We need to also include in the KB was remains \nunchanged\n - this is a well known issue called the \nrepresentation frame problem\n. It took its name from the video frame where most of the pixels stay unchanged (background) and only a few change (foreground). Imagine the bandwidth that we need to consume to transmit \nall\n video frame pixels. This is analogous to the KB - imagine the combinatorial explosion if we have to specify sentences for everything that stays the same at every time instant. As an example we will have to specify that the wumpus is alive at \n\\(t+1\\)\n as it was at \n\\(t\\)\n, that the agent continues to have one arrow when it didn’t use it and a myriad other intuitively obvious axioms.\n \n \nLogical Inference\n \nEdit this page\nView source\nReport an issue"
  },
  "doc-a62a883c34f430a7a271387f82fddb94": {
    "content": "The SARSA Algorithm – Engineering AI Agents\nEngineering AI Agents\nBOOK\nFoundations\nTraining Deep Networks\nPerception\nKinematics\nState Estimation\nLarge Language Models\nMultimodal Reasoning\nTask Planning\nGlobal Planning\nLocal Planning\nMarkov Decision Processes\nReinforcement Learning\nVLA Agents\nCOURSES\nIntroduction to AI\nAI for Robotics\nDeep Learning for Computer Vision\nDATA MINING - BEING PORTED\nMEDIA\nAI for Robotics\nABOUT ME\n \n \nReinforcement Learning\nThe SARSA Algorithm\nReinforcement Learning\nReinforcement Learning\nGeneralized Policy Iteration\nMonte-Carlo Prediction\nTemporal Difference (TD) Prediction\nMC vs. TD(0)\n\\(\\epsilon\\)\n-greedy Monte-Carlo (MC) Control\nThe SARSA Algorithm\nSARSA Gridworld Example\nOn this page\nSARSA - TD Learning\nEdit this page\nView source\nReport an issue\nReinforcement Learning\nThe SARSA Algorithm\nThe SARSA Algorithm\nSARSA implements a \n\\(Q(s,a)\\)\n value-based GPI and naturally follows as an enhancement from the \n\\(\\epsilon-greedy\\)\n policy improvement step of MC control.\nWe meet also here the familiar two steps:\nThe first is a technique for learning the Q-function via TD-learning that we have seen in the prediction section.\nThe second is a method for evolving the policy using the learned Q-function.\nSARSA - TD Learning\nIn the TD prediction section, we have met the TD prediction step for \n\\(V(s)\\)\n but for control we need to predict the \n\\(Q(s,a)\\)\n.\nThe TD(0), also known as single-step TD, tree for SARSA is shown below:\nsarsa-update-tree\nSARSA action-value backup update tree. Its name is attributed to the fact that we need to know the State-Action-Reward-State-Action before performing an update.\nFollowing the value estimate of \ntemporal difference (TD) learning\n, we can write the value update equation as:\n\\[Q(S,A) = Q(S,A) + \\alpha (R + \\gamma Q(S^\\prime, A^\\prime)-Q(S,A))\\]\nEffectively the equation above updates the Q function by \n\\(\\alpha\\)\n times the direction of the TD error. What SARSA does is basically the policy iteration diagram we have seen in the control above but with a twist. Instead of trying to evaluate the policy using episodes as in MC, SARSA does policy improvement on an estimate obtained \nover each time step\n significantly increasing the iteration rate - this is figuratively shown below:\nsarsa-policy-iteration\nSARSA on-policy control\nThe idea is to increase the frequency of the so called \n\\(\\epsilon\\)\n-greedy policy improvement step where we select with probability \n\\(\\epsilon\\)\n a random action instead of the action that maximizes the \n\\(Q(s,a)\\)\n function (greedy). We do so, in order to “hit” new states and therefore improve on the degree of exploration of our agent and as a result giving opportunities to the agent to reduce its variance \nand\n its bias.\nThe SARSA algorithm is summarized below:\nsarsa-on-policy-control-algorithm\nSARSA algorithm for on-policy control\n \n \n\\(\\epsilon\\)\n-greedy Monte-Carlo (MC) Control\nSARSA Gridworld Example\n \n \nEdit this page\nView source\nReport an issue"
  }
}